{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a79a5e6",
   "metadata": {},
   "source": [
    "# GridForecast: Electricity Demand Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a6890",
   "metadata": {},
   "source": [
    "GridForecast is an end-to-end electricity demand forecasting system built on real-time ENTSO-E operational data, combining time series analytics (ARIMA, SARIMA, Prophet, LSTM), ensemble modeling, automated monitoring, and continuous retraining to enable data-driven grid operations and energy trading decisions.\n",
    "\n",
    "From insights derived from temporal demand patterns and renewable energy variability, actionable forecasting models were developed and deployed via an interactive web application. The application provides a dashboard interface for grid operators and energy traders to visualize 24-hour demand predictions, confidence intervals, and real-time model performance to support operational planning and risk mitigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ebf19",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcabe42",
   "metadata": {},
   "source": [
    "The objective of the project is to predict the Electricity Demand in Germany. So we need to collect the data using API from the ENTSO-E Transparency Platform. \n",
    "ENTSO-E Transparency Platform gives access to electricity generation, transportation, and consumption data for the pan-European market. It is having the details from the countries like Austria, Belgium, Switzerland, Denmark, Germany, Spain, France, UK, Italy, Ireland, Luxembourg, the Netherlands, Norway, Portugal, and Sweden. \n",
    "\n",
    "In order to get access to API, we need to register in the website - \n",
    "[https://transparency.entsoe.eu/](https://transparency.entsoe.eu/). \n",
    "\n",
    "For complete details, kindly refer the link - \n",
    "[https://transparencyplatform.zendesk.com/hc/en-us/articles/12845911031188-How-to-get-security-token](https://transparencyplatform.zendesk.com/hc/en-us/articles/12845911031188-How-to-get-security-token). \n",
    "\n",
    "How to fetch the dataset is given in detail in the link - [https://transparencyplatform.zendesk.com/hc/en-us/articles/15696643163924-Request-Methods](https://transparencyplatform.zendesk.com/hc/en-us/articles/15696643163924-Request-Methods). \n",
    "\n",
    "\n",
    "Apart from this traditional methods, they are providing the library to fetch the data using API - [https://github.com/EnergieID/entsoe-py](https://github.com/EnergieID/entsoe-py). \n",
    "\n",
    "We will be using this library because of convenience. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84ca4e",
   "metadata": {},
   "source": [
    "## Import libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfebee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import time \n",
    "import requests\n",
    "from entsoe import EntsoePandasClient\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a0f06",
   "metadata": {},
   "source": [
    "## Loading the API Key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d579e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Loading the API key from env file \n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('ENTSOE_API_KEY')\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"ERROR: ENTSOE_API_KEY not found\")\n",
    "\n",
    "else:\n",
    "    print(f\"API Key loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9b789",
   "metadata": {},
   "source": [
    "# Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20673ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading batch 1: 2015-01-01 to 2016-01-01\n",
      "Downloading complete dataset for demand prediction from 20150101 to 20151231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (34947, 81)\n",
      "Downloading batch 2: 2016-01-01 to 2017-01-01\n",
      "Downloading complete dataset for demand prediction from 20160101 to 20161231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (35043, 81)\n",
      "Downloading batch 3: 2017-01-01 to 2018-01-01\n",
      "Downloading complete dataset for demand prediction from 20170101 to 20171231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (34947, 83)\n",
      "Downloading batch 4: 2018-01-01 to 2019-01-01\n",
      "Downloading complete dataset for demand prediction from 20180101 to 20181231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (26204, 79)\n",
      "Downloading batch 5: 2019-01-01 to 2020-01-01\n",
      "Downloading complete dataset for demand prediction from 20190101 to 20191231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (34947, 68)\n",
      "Downloading batch 6: 2020-01-01 to 2021-01-01\n",
      "Downloading complete dataset for demand prediction from 20200101 to 20201231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (35043, 68)\n",
      "Downloading batch 7: 2021-01-01 to 2022-01-01\n",
      "Downloading complete dataset for demand prediction from 20210101 to 20211231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (34947, 69)\n",
      "Downloading batch 8: 2022-01-01 to 2023-01-01\n",
      "Downloading complete dataset for demand prediction from 20220101 to 20221231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (34947, 66)\n",
      "Downloading batch 9: 2023-01-01 to 2024-01-01\n",
      "Downloading complete dataset for demand prediction from 20230101 to 20231231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (34947, 68)\n",
      "Downloading batch 10: 2024-01-01 to 2025-01-01\n",
      "Downloading complete dataset for demand prediction from 20240101 to 20241231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (35043, 66)\n",
      "Downloading batch 11: 2025-01-01 to 2026-01-01\n",
      "Downloading complete dataset for demand prediction from 20250101 to 20251231\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (34947, 66)\n",
      "Downloading batch 12: 2026-01-01 to 2026-02-18\n",
      "Downloading complete dataset for demand prediction from 20260101 to 20260217\n",
      "Fetching: load (attempt 1)\n",
      "Fetching: wind_and_solar_forecast (attempt 1)\n",
      "Fetching: intraday_wind_and_solar_forecast (attempt 1)\n",
      "Fetching: generation (attempt 1)\n",
      "Fetching: import (attempt 1)\n",
      "Fetching: generation_import (attempt 1)\n",
      "Data Frame created with shape : (4515, 64)\n",
      "\n",
      "Success! Saved 380477 rows.\n"
     ]
    }
   ],
   "source": [
    "# Class for downloading the dataset from the website using API. \n",
    "\n",
    "class GermanyElectricityDownloader:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"\n",
    "        Initialize with ENTSO-E API key \n",
    "        \"\"\"\n",
    "        self.client = EntsoePandasClient(api_key=api_key)        \n",
    "        self.time_zone = 'Europe/Berlin'\n",
    "        self.country_code_old = 'DE_AT_LU'  # 2015 - 2018\n",
    "        self.country_code_new = 'DE_LU'     # 2019 onwards\n",
    "\n",
    "    def _get_country_code(self, year):\n",
    "        \"\"\"Return the appropriate country code based on the year.\"\"\"\n",
    "    \n",
    "        return self.country_code_old if year <= 2018 else self.country_code_new\n",
    "\n",
    "    def _fetch_with_retry(self, name, func, retries=3, delay=5):\n",
    "        \"\"\"Fetch a single query with retry logic.\"\"\"\n",
    "        for attempt in range(1, retries + 1):\n",
    "            # Retrying the fetching to bypass the temporary network/API failure\n",
    "            try:\n",
    "                # print(f\"Fetching: {name} (attempt {attempt})\")\n",
    "                df = func()\n",
    "                time.sleep(1)  \n",
    "                return df\n",
    "            except Exception as e:\n",
    "                if attempt == retries:\n",
    "                    print(f\"Failed to fetch '{name}' after {retries} attempts: {e}\")\n",
    "                    return None\n",
    "                print(f\"Attempt {attempt} failed for '{name}': {e}. Retrying in {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def download_all(self, start_date, end_date):\n",
    "        \"\"\"Download complete dataset for demand prediction\n",
    "        \n",
    "        Args:\n",
    "            start_date : Starting date for which data need to be fetched. \n",
    "            end_date : Ending date for which data need to be fetched. \n",
    "        Return: \n",
    "            Combined DataFrame will returned by concatenating all the DataFrame generated. \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        print(f\"Downloading complete dataset for demand prediction from {start_date} to {end_date}\")\n",
    "        \n",
    "        try:\n",
    "            start = pd.Timestamp(start_date, tz=self.time_zone)\n",
    "            end = pd.Timestamp(end_date, tz=self.time_zone) + pd.Timedelta(minutes=30)\n",
    "\n",
    "            start_year = start.year\n",
    "            end_year = end.year\n",
    "\n",
    "            all_year_dfs = []\n",
    "\n",
    "            for year in range(start_year, end_year + 1):\n",
    "                country_code = self._get_country_code(year)\n",
    "                year_start = max(\n",
    "                        pd.Timestamp(f'{year}-01-01', tz=self.time_zone),\n",
    "                        start\n",
    "                    )\n",
    "                year_end = min(\n",
    "                        pd.Timestamp(f'{year+1}-01-01', tz=self.time_zone),\n",
    "                        end\n",
    "                    )\n",
    "\n",
    "                # Code from https://github.com/EnergieID/entsoe-py library for downloading the data via API\n",
    "                time_series_queries = {\n",
    "                    \"load\": lambda: self.client.query_load(\n",
    "                        country_code, start=year_start, end=year_end),\n",
    "\n",
    "                    \"wind_and_solar_forecast\": lambda: self.client.query_wind_and_solar_forecast(\n",
    "                        country_code, start=year_start, end=year_end, psr_type=None),\n",
    "\n",
    "                    \"intraday_wind_and_solar_forecast\": lambda: self.client.query_intraday_wind_and_solar_forecast(\n",
    "                        country_code, start=year_start, end=year_end, psr_type=None),\n",
    "\n",
    "                    \"generation\": lambda: self.client.query_generation(\n",
    "                        country_code, start=year_start, end=year_end, psr_type=None),\n",
    "\n",
    "                    \"import\": lambda: self.client.query_import(\n",
    "                        country_code, start=year_start, end=year_end),\n",
    "\n",
    "                    \"generation_import\": lambda: self.client.query_generation_import(\n",
    "                        country_code, year_start, year_end),\n",
    "                }\n",
    "\n",
    "\n",
    "                dfs = []\n",
    "\n",
    "                for name, func in time_series_queries.items():\n",
    "                    df = self._fetch_with_retry(name, func)\n",
    "\n",
    "                    if df is None:\n",
    "                        continue\n",
    "\n",
    "                    df = df.sort_index()\n",
    "                    if df.index.tz is None:\n",
    "                        df.index = df.index.tz_localize(\"UTC\").tz_convert(self.time_zone)\n",
    "                    else:\n",
    "                        df.index = df.index.tz_convert(self.time_zone)\n",
    "\n",
    "                    if isinstance(df.columns, pd.MultiIndex):\n",
    "                        df.columns = [\"_\".join(col).strip() for col in df.columns]\n",
    "\n",
    "                    # Adding prefix so we get separate column names \n",
    "                    df = df.add_prefix(f\"{name}_\")\n",
    "\n",
    "                    dfs.append(df)\n",
    "\n",
    "                if dfs:\n",
    "                    year_df = pd.concat(dfs, axis=1, sort=False)\n",
    "                    all_year_dfs.append(year_df)\n",
    "\n",
    "            if not all_year_dfs:\n",
    "                return None\n",
    "\n",
    "            # Combining all DataFrame to one DataFrame \n",
    "            final_df = pd.concat(all_year_dfs)\n",
    "            final_df = final_df[~final_df.index.duplicated(keep='last')]\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"Data Frame created with shape : {final_df.shape}\")\n",
    "            return final_df\n",
    "        except ConnectionError as e:\n",
    "            print(f\"Network error: {e}\")\n",
    "            return None\n",
    "\n",
    "        except pd.errors.EmptyDataError as e:\n",
    "            print(f\"No data returned from API: {e}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error downloading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def update_data(self): \n",
    "        \"\"\"Check the availability of the dataset if available update the missing and if not download the complete historical data.\"\"\"\n",
    "\n",
    "        \n",
    "        # Creating the artifacts folder for saving the csv file. \n",
    "        project_root = Path.cwd().parent\n",
    "        output_dir = project_root / 'artifacts/raw'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Defining the csv name \n",
    "        csv_filename = 'power_consumption_germany.csv'\n",
    "        csv_file_path_str = str(output_dir / csv_filename)\n",
    "\n",
    "        # Once the data is loaded from the website using API and when we are updating the new data we don't want to download the data again. \n",
    "        # So the check is created for checking the data and then downloading the data which pending for update. \n",
    "        if os.path.isfile(csv_file_path_str):\n",
    "            # File exists then check the last date in the CSV\n",
    "            existing_df = pd.read_csv(csv_file_path_str, index_col=0, parse_dates=True)\n",
    "            existing_df.index = pd.to_datetime(existing_df.index, utc=True).tz_convert('Europe/Berlin')\n",
    "\n",
    "            last_date = existing_df.index.max().date()\n",
    "            today = date.today()\n",
    "            yesterday = today - timedelta(days=1)\n",
    "\n",
    "            if last_date >= yesterday:\n",
    "                print(f\"CSV is already up to date (last record: {last_date}). Nothing to download.\")\n",
    "                return\n",
    "\n",
    "            # Download missing days from day after last record to yesterday\n",
    "            fetch_start = last_date + timedelta(days=1)\n",
    "            fetch_end = yesterday \n",
    "\n",
    "            print(f\"CSV exists but is outdated. Fetching missing data: {fetch_start} to {yesterday}\")\n",
    "\n",
    "            new_data = self.download_all(\n",
    "                start_date=fetch_start.strftime('%Y%m%d'),\n",
    "                end_date=fetch_end.strftime('%Y%m%d')\n",
    "            )\n",
    "            # Adding the Data with already available data\n",
    "            if new_data is not None and not new_data.empty:\n",
    "                updated_df = pd.concat([existing_df, new_data])\n",
    "                updated_df = updated_df[~updated_df.index.duplicated(keep='last')]  # safety dedup\n",
    "                updated_df.to_csv(csv_file_path_str)\n",
    "                print(f\"Updated CSV with {len(new_data)} new rows. Total rows: {len(updated_df)}\")\n",
    "            else:\n",
    "                print(\"No new data was returned from the API.\")\n",
    "\n",
    "        else:\n",
    "            # File does not exist then do the full historical backfill\n",
    "            start = pd.Timestamp('20150101', tz=self.time_zone)\n",
    "            end = pd.Timestamp(date.today().strftime('%Y%m%d'), tz=self.time_zone)\n",
    "\n",
    "            batch = 0\n",
    "            all_data = []\n",
    "            \n",
    "            # Downloading the data in batches to avoid API failures \n",
    "            while start < end:\n",
    "                batch += 1\n",
    "                batch_end = min(start + relativedelta(years=1), end)\n",
    "                print(f\"Downloading batch {batch}: {start.date()} to {batch_end.date()-timedelta(days=1)}\")\n",
    "                \n",
    "                data = self.download_all(\n",
    "                    start_date=start.strftime('%Y%m%d'),\n",
    "                    end_date=(batch_end - timedelta(days=1)).strftime('%Y%m%d')\n",
    "                )\n",
    "                \n",
    "                if data is not None and not data.empty:\n",
    "                    all_data.append(data)\n",
    "                \n",
    "                start = batch_end\n",
    "\n",
    "            # Adding the full data to created a combined final dataset \n",
    "            if all_data:\n",
    "                final_df = pd.concat(all_data)\n",
    "                final_df = final_df[~final_df.index.duplicated(keep='last')]\n",
    "                final_df.to_csv(csv_file_path_str)\n",
    "                print(f\"\\nSuccess! Saved {len(final_df)} rows.\")\n",
    "            else:\n",
    "                print(\"No data was downloaded.\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    downloader = GermanyElectricityDownloader(api_key=API_KEY)\n",
    "\n",
    "    data = downloader.update_data()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grid-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
